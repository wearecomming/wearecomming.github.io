{"pages":[{"title":"About Me","text":"","link":"/about/index.html"}],"posts":[{"title":"Building Hexo Blogs in Multiple Languages","text":"Installation of HexoThis section refers to Article Building multiple language themesThis section refers to Article","link":"/2023/10/18/Building%20Hexo%20Blogs%20in%20Multiple%20Languages/"},{"title":"Summary and Techniques of RAG (Retrieval Augmented Generation)","text":"RAG is a fast text search technique that aims to search for the most relevant paragraphs with keywords in a large amount of text based on the given key words and phrases. Due to its characteristics, RAG can often provide the latest knowledge to large language models. Large language models, such as chatGPT, are unable to obtain the latest information and some more confidential information. But if we need the reasoning ability of the big language model to process this confidential information, we need to use RAG to provide this information to the big language model, and then use the big language model to help us analyze and summarize information, achieving the function of improving efficiency. Due to token limitations, we are unable to input all our information into the large language model at once. Therefore, every time we ask a question to the big language model, we must find the most relevant information to our question in a massive amount of information files and provide it to the big language model. Then, the big language model can summarize the rules or refine them into a few sentences. The process of finding information is what RAG does. RAG can be built using the Langchain framework or by oneself. The process of RAG is to first divide a large amount of information text into several text blocks for querying, and then convert these text blocks into text vector vectors through natural language encoding models such as Bert (which is similar to facial recognition face vectors). Then, these text blocks and corresponding text vectors are stored in a vector database for fast searching. A vector database is a database that can quickly search for similar vectors. It is recommended to use the Fairs database for vector databases. Although it is difficult to deploy, GPU acceleration can be used, which is much faster than regular databases. If you want to consider data at the level of millions, you basically use Fairs or milvus. If you are only considering quick construction, you can use chroma, which is simple and easy to get started. By the way, the Langchain framework has a component called faiss, so faiss can be easily used within Langchain. After being stored in the vector database, preprocessing ends and we move on to the query phase. In the query phase, we first use the same natural language encoding model to convert the question we want to ask into a vector, and then we can query the text vector closest to our question vector in the vector database. After finding this text vector, you can find the corresponding text block. This text block is the information text we need to provide to the big language model. At this point, RAG is almost over. There are several tips for RAG: RAG not only allows for single search, but also adopts a parent-child document block search mode for accuracy. When analyzing large languages, it is best to do it step by step and break it down into smaller tasks. Due to the fact that the answers of the big language model are based on probability and are very unstable, some answers to the same question may be very outrageous. To avoid generating outrageous answers, it is best to ask the big model multiple times with the same prompt words to get multiple answers, as there are always more correct answers than incorrect ones. Then, using a large model to summarize these answers can effectively avoid incorrect answers. If it’s still wrong, you need to check the prompt words (smile). You can refer to a similar approach to intelligent agents and say “you are now a…, you need to do…” to a large model, which is better than directly commanding the large model. For example, “you are now a database operator, and you need to perform text extraction on the problem”, which is better than directly asking the large model to do text extraction.","link":"/2024/04/25/Summary%20and%20Techniques%20of%20RAG%20(Retrieval%20Augmented%20Generation)/"}],"tags":[],"categories":[]}